Ранние методы:
1. **PointNet**
    - Обрабатывает неупорядоченные точки напрямую через симметричные функции.
    - Рассматривает каждую точку независимо, не учитывая локальную геометрию.
2. **PointNet++**
    - Улучшение PointNet, объединяющее локальные многомасштабные признаки через абстракционный слой.
    - Признаки соседних точек, находящихся в одинаковой локальной области, почти одинаковы.
    - Перестановочно-инвариантные сети, такие как PointNet и PointNet++, лучше захватывают локальные геометрические структуры, что улучшает вывод по точечным облакам.

Недавние подходы:
- **PAN (Point Atrous Network)**
    - Вводит **Point Atrous Convolution (PAC)**, которая увеличивает поле восприятия фильтров за счёт параметра **sampling rate**, позволяющего разреженно выбирать соседние точки.
    - Позволяет эффективно изучать **многомасштабные признаки рёбер**.
    - Однако PAN **не имеет иерархической кодировщик-декодер структуры**, что снижает эффективность при обработке плотных и высокоразмерных точечных признаков.


**Анализ точечных облаков.**
**PointNet** достигая перестановочной инвариантности с помощью **симметричных функций**. Его давно используют но другие модели более сложные симметричные операции.
Для решения этой задачи:
- **SPG** и **SGPN** строят графы супер-точек, чтобы уточнять результаты семантической разметки.
- **PAN** вводит **Point Atrous Convolution (PAC)** для эффективного использования **многомасштабных локальных признаков рёбер**
Однако, в отличие от многих сетей для семантической разметки изображений, эти методы **не используют иерархические кодировщик-декодер архитектуры**, что ограничивает их производительность.
**Иерархический кодировщик-декодер.**
Глубокие иерархические архитектуры кодировщик-декодер широко применяются в задачах обработки изображений:
- Оценка позы человека
- Семантическая сегментация
- Оценка оптического потока
- Обнаружение объектов
Особенности таких архитектур:
- **Stacked hourglass modules**: последовательные операции pooling и upsampling, дают высокое качество, например, в оценке позы человека. 
- **Feature Pyramid Network**: используется для обнаружения объектов с многомасштабными признаками.
- **U-Net и DeconvNet**: симметричные кодировщик-декодер архитектуры, улучшающие маски сегментации за счёт использования признаков низкоуровневых слоёв.
- **DeepLabv3+**: сочетает архитектуру кодировщик-декодер с атросной свёрткой, что позволяет **эффективно менять поле восприятия фильтров и захватывать многомасштабный контекст**, достигая новых рекордных результатов на задачах сегментации.
**Типичная структура глубокого иерархического кодировщик-декодера включает:**
1. **Encoder**: постепенно уменьшает разрешение признаков, увеличивает поле восприятия фильтров и захватывает высокоуровневую семантическую информацию.
2. **Decoder**: постепенно восстанавливает пространственную информацию для точного вывода.

PointAtrousGraph (PAG)**:
 это глубокая иерархическая архитектура кодировщик-декодер, которая предназначена для изучения многомасштабных признаков рёбер (edge features) в 3D-точечных облаках.
- Глубокий, перестановочно-инвариантный **иерархический кодировщик-декодер** для анализа 3D-точек.
- Использует **PAC-модули** для изучения локальных многомасштабных геометрических деталей.
- Для сохранения критических признаков рёбер при уменьшении размерности вводится **Edge-preserved Pooling (EP)**.
- Для восстановления пространственной информации после уменьшения размерности — **Edge-preserved Unpooling (EU)**.
- Введены **цепочки skip-субсемплинга/апсемплинга**, которые напрямую передают признаки точек между уровнями.
- Новые **вспомогательные функции потерь**, включая **MMD loss** и **deeply supervised loss**, повышают точность предсказаний.
- PAG потребляет **меньше памяти и требует меньше времени на обучение** по сравнению с другими сетями, сильно зависящими от локальных графов соседства.
### **A. Point Atrous Convolution (PAC) для работы с плотными признаками**
**Проблема традиционных методов поиска соседей** - они берут соседние точки самые близкие и используют ограниченный радиус, а он смотрит как бы через одну точку 
$$X'_p = g\Big( H_\Theta(X_p,\, X^q_{1\cdot r}),\, H_\Theta(X_p,\, X^q_{2\cdot r}),\, \dots,\, H_\Theta(X_p,\, X^q_{k\cdot r}) \Big)$$
- Xp​ — признак центроидной точки p (размер C );
- Xk⋅rq​ — признак **(k⋅r) -го ближайшего соседа** точки p (не k -го!);
- r — **шаг выборки** (_sampling rate_), r∈{1,2,4,…} ;  
    → r=1 : обычный kNN-фильтр;  
    → r=2 : «прореженная» выборка — больший охват при том же k ;
- k — число **выбираемых соседей** (например, k=5 → берём 5-ю, 10-ю, 15-ю и т.д. точки);
- HΘ​(⋅) — *краевое ядро (edge kernel):
    
    latex
$$H_\Theta(X_p, X_q) = h_\theta\!\left( X_p \oplus (X_p - X_q) \right)$$
- - ⊕ — конкатенация;
    - Xp​−Xq​ — разность признаков (аналог геометрического смещения);
    - hθ​ — **shared MLP** (одинаковая для всех пар);
- g(⋅) — агрегирующая функция, в работе — **max-pooling** по k значениям.
### **B. Edge-Preserved Pooling (EP) — пулинг с сохранением краёв**
Проблема в том что есть перекрывающие графы

- Выбираются «опорные» точки (центроиды);
- Для каждой строится окрестность (граф соседства) как бы кеширует в них данные рядом;
- Но соседние центроиды часто имеют перекрывающиеся окрестности → их закодированные признаки становятся **почти одинаковыми** → **потеря индивидуальности точек**, особенно в глубоких сетях.
-После происходит сжатие и тем самым повышает различимостть признаков 
$$X'_p = X_p \oplus g\!\left( X^q_1,\, X^q_2,\, \dots,\, X^q_k \right)$$
- ⊕ — конкатенация;
- _g_ — max-pooling (агрегация соседей);
- Xkq​ — признаки _k_ ближайших соседей в **исходном** облаке (до субдискретизации!).
### **C. Edge-Preserved Unpooling (EU) — восстановление структуры**
Потом происходит восстановление структуры с учетом геометрий
$$X'_p = X^e_p \oplus w\!\left( X^q_1,\, X^q_2,\, \dots,\, X^q_k \right)$$
- Xpe​ — признак точки p , полученный **из энкодера** через skip-connection;
- X1q​,…,Xkq​ — признаки k ближайших соседей точки p **на предыдущем (более плотном) уровне иерархии**;
- w(⋅) — **взвешенное усреднение с обратным расстоянием**
$$w = \sum_{i=1}^k \alpha_i \cdot X^q_i, \quad \text{где} \quad \alpha_i = \frac{1/d_i}{\sum_j 1/d_j}, \quad d_i = \| \mathbf{p} - \mathbf{q}_i \|$$
- p,qi​ — **пространственные координаты** (только для весов!);
- Признаки Xiq​ — берутся из сети, **не из координат**.
Таким образом EU аккуратно восстанавливает плотность, комбинируя перенесённые низкоуровневые признаки и интерполированные локальные признаки.
![[Pasted image 20251117000119.png]] Благодаря строиться такая структура 
### **E. Вспомогательные функции потерь**

Для сегментации вводятся две дополнительные функции потерь:

#### 1. **Maximum Mean Discrepancy (MMD) Loss**
- Используется в вариационных автокодировщиках для согласования распределений.
- Цель: сделать распределение глобальных признаков **близким к стандартному нормальному** N(0,1) .
- Формула (с ядерным трюком)
- $$\mathcal{L}_{\text{mmd}} = 
\mathbb{E}_{\mathbf{z},\mathbf{z}' \sim q} \big[ k(\mathbf{z}, \mathbf{z}') \big] 
+ \mathbb{E}_{\mathbf{z},\mathbf{z}' \sim p} \big[ k(\mathbf{z}, \mathbf{z}') \big] 
- 2 \mathbb{E}_{\mathbf{z} \sim q,\, \mathbf{z}' \sim p} \big[ k(\mathbf{z}, \mathbf{z}') \big]$$
- q(z) — эмпирическое распределение **глобальных признаков** (после max-pooling);
- p(z)=N(0,1) — целевое распределение;
- k(⋅,⋅) — положительно определённое ядро (в работе — **RBF**)
$$k(\mathbf{z}, \mathbf{z}') = \exp\!\left( -\frac{\| \mathbf{z} - \mathbf{z}' \|^2}{2\sigma^2} \right)$$- Lmmd​≥0 , и Lmmd​=0⟺q=p ;
- Не требует сэмплирования из q — вычисляется по батчу (аналогично в VAE/GAN);
- Улучшает обобщение и стабильность обучения.

#### 2. **Deeply Supervised (DS) Loss**

- Дополнительная **cross-entropy-потеря** на **первом уровне декодера** (грубое предсказание).
- Даёт градиенты «раньше» — помогает обучать глубокие сети.
$$\mathcal{L}_{\text{all}} = 
\mathcal{L}_{\text{master}} 
\;+\; w_{\text{mmd}} \cdot \mathcal{L}_{\text{mmd}} 
\;+\; w_{\text{ds}} \cdot \mathcal{L}_{\text{ds}}$$
- Lmaster​ — основная **cross-entropy loss** на финальном выходе;
- Lds​ — **deeply supervised loss**: cross-entropy на **первом уровне декодера** (грубое предсказание);
- wmmd​,wds​ — гиперпараметры баланса (в статье подбираются эмпирически).
**Эксперименты**
**PAG тестируют на трёх задачах:**
1. **Классификация форм** – определение категории 3D-объекта (например, стул, стол, машина).
2. **Сегментация частей объекта** – определение, какие точки принадлежат какой части объекта (например, ножка стула, сиденье).
3. **Семантическая сегментация** – определение классов объектов в больших сценах (например, мебель, стены, пол в комнате).
- **Классификация форм (ModelNet40)** – точность **93.8%**, выше, чем у PAN и SO-Net (**93.4%**).
- **Сегментация частей объектов (ShapeNet-part)** – mean IoU **86.4%**, выше, чем у A-CNN (**86.1%**).
- **Семантическая сегментация (S3DIS)** – mIoU **65.9% / 59.3%**, лучше, чем у PointCNN и PCCN (**65.4% / 58.3%**).
PAG показывает лучшие или сравнимые результаты с меньшей памятью и временем обучения, остаётся устойчивой к неполным данным и не требует дополнительных постобработок.
### **Выводы**
Представлена архитектура **PointAtrousGraph (PAG)** для анализа 3D-точечных облаков.
PAG эффективно **извлекает многомасштабные локальные геометрические признаки** благодаря новым модулям:
    **PAC (Point Atrous Convolution)** – выбор соседей с увеличенным полем восприятия без потери плотности признаков.    
    **EP (Edge-Preserved Pooling)** – иерархическое суммирование признаков с сохранением краевых деталей.  
Архитектура **энкодер-декодер** позволяет:
    Энкодер – уменьшает плотность признаков, увеличивает контекст
	Декодер – восстанавливает плотность для точной сегментации.
- PAG пермутационно-инвариантна – результаты не зависят от порядка точек.
- Работает быстрее и экономичнее по памяти, чем конкуренты (PointNet++, DGCNN, SpiderCNN).
    
### **Сравнение с другими подходами**
- **Воксельные представления** (VoxelNet, VoxNet) - прямой способ работы с 3D, но требуют много памяти и плохо масштабируются.
- **Иерархические структуры** (kd-tree, octree) - эффективны для sparse-данных, но сложны в реализации.
- **Прямо с точками** (PointNet, PointCNN) - гибко, но есть два подхода:
    - Симметричные функции (PointNet, PAG) - пермутационно-инвариантны.
    - Canonical ordering (PointCNN) - сложно реализовать, чувствительно к порядку точек, результаты могут быть нестабильны.
### **Особенности PAG**
- Комбинация PAC + EP позволяет извлекать локальные признаки разных масштабов **эффективно и точно**.
- Использует соседей как в **feature space**, так и в **metric space** для разных целей:
    - Feature space → динамические соседние точки, лучше для обучения признаков.
    - Metric space → статические соседние точки, стабилизируют иерархию и операции unpooling.
- Архитектура устойчива к плотности и распределению точек, может работать с большими 3D сценами.
![[1907.09798v2 (1).pdf]]![[1907.09798v2.pdf]]