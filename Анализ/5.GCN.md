это нейросети, работающие с графами, использующие идею свёрток, как в CNN, но адаптированную под структуру графа.  
**GCN свёртка делается не по картинке, а по соседям каждого узла**, берутся только ближайшие (первый порядок).

---

**Полу-контрольное обучение**
$$
\mathcal{L} = \mathcal{L}_0 + \lambda \, \mathcal{L}_{\text{reg}}
$$ Полная функция потерь где 
Контролируемая потеря $$\mathcal{L}_0$$ находится по формуле
​
$$
\mathcal{L}_0 = - \sum_{i \in Y} y_i \log p_i
$$
где yi это известная метка узла известка 
pi - это предсказанная вероятность модели
и находим их сумму 
Lreg это Регуляризация
$$\mathcal{L}_{\text{reg}} = \sum_{i,j} A_{ij} \| f(X_i) - f(X_j) \|^2$$
Находим связи между узлами вычитаем из их находим сумму разности квадратов их значений по одной координате а потом суммируем все связи между узлами 
λ-**коэффициент, который регулирует важность одной части потери относительно другой**.
λ

- **Большое λ «важнее, чтобы соседние узлы были похожи, даже если точность на размеченных узлах чуть хуже»
    
- **Малое λ  «важнее точно предсказывать известные метки, а соседей можно не сильно сглаживать»
ПРИМЕР
мы имеем матрицу смежности 
​0 1 1 0​
1 0 1 1
1 1 0 0
0​ 1 0 0
и метки для нее
для 1 узла 0.1
для 2 узла 1.0
для 3 4 ?
модель начинает предсказывать 
|1|[0.8, 0.2]|
|2|[0.1, 0.9]|
|3|[0.5, 0.5]|
|4|[0.3, 0.7]|
Контролируемая потеря L0 = −(y1 ​log p1​+y2​ log p2​)
Узел 1: y1​=[1,0] p1​=[0.8,0.2] == 0.223
Узел 2: y2=[0,1], p2=[0.1,0.9] == 0.105
L0​=0.223+0.105=0.328
Регуляризация Lreg
Узлы 1 и 2 связаны
(0.8−0.1)2+(0.2−0.9)2=0.49+0.49=0.98
Узлы 1 и 3 связаны
(0.8−0.5)2+(0.2−0.5)2=0.09+0.09=0.18
Узлы 2 и 3
(0.1−0.5)2+(0.9−0.5)2=0.16+0.16=0.32
Узлы 2 и 4
(0.1−0.3)2+(0.9−0.7)2=0.04+0.04=0.08
Lreg​=0.98+0.18+0.32+0.08=1.56
Полная функция потерь
Пусть λ=0.5, тогда:
L=0.328+0.5⋅1.56=0.328+0.78=1.108
те мы получили ошибку 1.108 меньше лучше те модель должна перебирать вероятности пока не минимизирует ее 

---

**Основная формула слоя GCN**
$$
H^{(l+1)} = \sigma\big(\widetilde{D}^{-1/2} \widetilde{A} \, \widetilde{D}^{-1/2} H^l W^l \big)
$$

Где:

-$$ (H^l) $$— матрица признаков узлов на слое \(l\) (\(N \times F_l\))  
- $$(W^l) $$— обучаемая матрица весов слоя (\(F_l \times F_{l+1}\))  
-$$ (\sigma) $$— функция активации (ReLU, sigmoid, tanh)  
- $$(\widetilde{A} = A + I_N) $$— матрица смежности с добавленными самопетлями  
- $$(\widetilde{D}) $$— диагональная матрица степеней: $\$(\widetilde{D}_{ii} = \sum_j \widetilde{A}_{ij}\$)$
$$
\widetilde{A} = A + I_N
$$

- Добавление \(I_N\)(единичная матрица) позволяет узлам учитывать свои собственные признаки при агрегации. 
$$
\widetilde{D}^{-1/2} \widetilde{A} \widetilde{D}^{-1/2}
$$

- Используется для нормализации, чтобы узлы с разным количеством соседей не доминировали, что бы исключить доминацию узлов с большим количеством соседей и не забывать про малых.
- **Матрица смежности AAA**:$$
A = 
\begin{bmatrix}
0 & 1 & 0 \\
1 & 0 & 1 \\
0 & 1 & 0
\end{bmatrix}
$$
**Признаки узлов H0H^0H0**:$$
H^0 = 
\begin{bmatrix}
1 \\
2 \\
3
\end{bmatrix}
$$Добавляем самопетли:
$$
\widetilde{A} = A + I_3 =
\begin{bmatrix}
1 & 1 & 0 \\
1 & 1 & 1 \\
0 & 1 & 1
\end{bmatrix}
$$
Степени узлов и нормализация:
$$
\widetilde{D} = 
\begin{bmatrix}
2 & 0 & 0 \\
0 & 3 & 0 \\
0 & 0 & 2
\end{bmatrix}, \quad
\widetilde{D}^{-1/2} \widetilde{A} \widetilde{D}^{-1/2} =
\begin{bmatrix}
1/\sqrt{2} & 1/\sqrt{2} & 0 \\
1/\sqrt{3} & 1/\sqrt{3} & 1/\sqrt{3} \\
0 & 1/\sqrt{2} & 1/\sqrt{2}
\end{bmatrix}
$$
**Обновление признаков узлов**
$$
H^1 = \widetilde{D}^{-1/2} \widetilde{A} \widetilde{D}^{-1/2} H^0
$$

Расчёт по узлам:

- Узел 1: $$(h_1^1 = 1/\sqrt{2} \cdot 1 + 1/\sqrt{2} \cdot 2 \approx 2.12)  $$
- Узел 2: $$(h_2^1 = 1/\sqrt{3} \cdot 1 + 1/\sqrt{3} \cdot 2 + 1/\sqrt{3} \cdot 3 \approx 3.46) $$ 
- Узел 3: $$(h_3^1 = 1/\sqrt{2} \cdot 2 + 1/\sqrt{2} \cdot 3 \approx 3.54)$$
мы получили новые признаки
Исходные признаки это информация только о самом узле, без учёта структуры графа
Новые признаки дают - информацию о своих соседях.

---

То есть **GCN — это практическая реализация спектральной свёртки через Чебышёвскую аппроксимацию** 
Нормализованный Лапласиан графа:

$$
L = I_N - D^{-1/2} A D^{-1/2} = U \Lambda U^T
$$

Спектральная свёртка сигнала $x$:

$$
g_\theta \ast x = U g_\theta(\Lambda) U^T x
$$

Аппроксимация через полином Чебышева:

$$
g_\theta(\Lambda) \approx \sum_{k=0}^{K} \theta_k' T_k(\tilde{\Lambda}), \quad \tilde{\Lambda} = \frac{2}{\lambda_{\max}} \Lambda - I_N
$$

Упрощённое приближение для 1-го слоя (K=1):

$$
g_\theta \ast x \approx \theta_0' x - \theta_1' D^{-1/2} A D^{-1/2} x
$$

С одним параметром:

$$
g_\theta \ast x \approx \theta \left(I_N + D^{-1/2} A D^{-1/2}\right) x
$$
---

ДУХСЛОЙНАЯ МОДЕЛЬ GCN для полу-контролируемой классификации узлов
Вот аккуратная подборка **формул GCN**, которые можно вставлять в Obsidian с использованием синтаксиса `$$ ... $$` для красивого отображения LaTeX. Я также поясню, что каждая формула делает.

###Нормализация матрицы смежности

$$  
\tilde{A} = A + I  
$$

$$  
\hat{A} = D^{-1/2} \tilde{A} D^{-1/2}  
$$
добовляем еденчнуб и нормализуем через -1/2 D это строка сумма связей в матрицк
- (A) — исходная матрица смежности
    
- (I) — единичная матрица (самопетли)
    
- (D) — диагональная матрица степеней узлов ((D_{ii} = \sum_j \tilde{A}_{ij}))
    
- ({A}) — нормализованная матрица смежности, чтобы узлы с большим количеством соседей не доминировали
    

### Первый слой (скрытый)

$$  
H^{(1)} = \mathrm{ReLU}(\hat{A} X W^{(0)})  
$$
 
- (X) - признаки узлов ((N \times C))
    
- W(0) - обучаемая матрица весов первого слоя ((C \times H))
    
- ReLU - функция активации (убирает отрицательные значения) убрать минусы что ыб не мешались
    
- A^XW(0) - агрегирование признаков соседей + линейное преобразование
    
берется матрицв весов какя 2 скрытых 2 класа к примеру 2 на 2 далее вычесляем релю для каждого узлов из перемножение матрицы полученную выше на признаки 
###  Второй слой (выходной)

$$  
Z = \mathrm{softmax}(\hat{A} H^{(1)} W^{(1)})  
$$

- W(1) - матрица весов второго слоя ((H \times F))
    
- Softmax - преобразует выход в **вероятности классов**, сумма по строке = 1 что бы значение были равны 1 и предсавляли из себя вероятсность 
    
- A^H(1)W(1) - повторное агрегирование признаков от соседей
    

находим софт Макс то есть  экспоненту в мощном значение деленую на сумму экспонент мощного и обычного крч все делаем в сумму 1 и получим новые предсказания класса 
### Функция потерь (Cross-Entropy)

$$  
\mathcal{L} = - \sum_{l \in \mathcal{Y}_\mathcal{L}} \sum_{f=1}^{F} Y_{lf} \log Z_{lf}  
$$

-YL​ - множество узлов с известными метками
    
-Ylf - индикаторная переменная (1, если узел (l) принадлежит классу (f))
    
-Zlf​ - предсказанная вероятность класса (f) для узла (l)
    
- Считаем ошибку только по **узлам с метками** (полу-контролируемый режим)
    

### Полная формула двухслойной GCN

$$  
\boxed{  
Z = \mathrm{softmax} \Big( \hat{A} , \mathrm{ReLU}(\hat{A} X W^{(0)}) W^{(1)} \Big)  
}  
$$

- На вход подаём признаки (X) и матрицу смежности (A)
    
- Выход - вероятности классов для всех узлов

Первая формула — это «идея свертки на графе» в простейшей форме
    
Вторая — реальная формула GCN, которая используется на практике, потому что **учитывает степени узлов и добавляет себя как сосед**









