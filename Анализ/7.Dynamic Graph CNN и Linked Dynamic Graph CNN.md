

### Dynamic Graph CNN

Используется не ручное а машинное обучение и в отличие Полин нет анализирует не только точку а из соседей
![[Pasted image 20251128004630.png]]
A: [0.7, 0.9, -0.2, 1.2, ..., 64 шт]
A: 64-dim
B: 64-dim
C: 64-dim
D: 64-dim
dist(A, B) = ||A_64 - B_64||₂
dist(A, C) = ||A_64 - C_64||₂
...
A → C, D
B → D, A
C → A, D
D → C, B
так мы делаем сверху и снизу и повторяем мегментацию и классификацию как в поинт нет 
веса для FC находится в процессе обучения 
### EdgeConv
 
Исходные данные
Возьмем упрощенный пример в 2D для наглядности. Пусть у нас есть 3 точки:
· Главная точка: x₁ = [1, 2]
· Сосед 1: x₂ = [1, 3]
· Сосед 2: x₃ = [2, 2]
Параметры k = 2 (ищем 2 ближайших соседа).
Шаг 1: Построение графа
Строим k-NN граф. Для точки x₁ находим 2 ближайших соседа - это x₂ и x₃.
Шаг 2: Вычисление реберных признаков
Используем асимметричную функцию из статьи:
eᵢⱼ = ReLU(θ·(xⱼ - xᵢ) + φ·xᵢ)
Возьмем конкретные веса:
 θ = [0.5, -0.3] (для локальной геометрии)
 φ = [0.2, 0.4] (для глобальной позиции)
Для ребра (1→2):
x₂ - x₁ = [1-1, 3-2] = [0, 1]
Вычисляем:
```

θ·(x₂ - x₁) = 0.5×0 + (-0.3)×1 = -0.3

φ·x₁ = 0.2×1 + 0.4×2 = 0.2 + 0.8 = 1.0

Сумма: -0.3 + 1.0 = 0.7

ReLU(0.7) = 0.7

```
Итог: e₁₂ = 0.7
Для ребра (1→3):
x₃ - x₁ = [2-1, 2-2] = [1, 0]
Вычисляем:
```

θ·(x₃ - x₁) = 0.5×1 + (-0.3)×0 = 0.5

φ·x₁ = 0.2×1 + 0.4×2 = 1.0  

Сумма: 0.5 + 1.0 = 1.5

ReLU(1.5) = 1.5

```
Итог: e₁₃ = 1.5
Для self-loop (1→1):
x₁ - x₁ = [0, 0]
Вычисляем:
```

θ·(x₁ - x₁) = 0.5×0 + (-0.3)×0 = 0

φ·x₁ = 1.0

Сумма: 0 + 1.0 = 1.0

ReLU(1.0) = 1.0

```
Итог: e₁₁ = 1.0
Шаг 3: Агрегация (Max Pooling)
Теперь агрегируем все реберные признаки, исходящие из точки 1:
x'₁ = MAX(e₁₁, e₁₂, e₁₃) = MAX(1.0, 0.7, 1.5) = 1.5
Шаг 4: Результат
Исходная точка: x₁ = [1, 2]
После EdgeConv: x'₁ = 1.5
Потом по этим данным мы можем построить новый граф динамичекски его обновив 
был к примеру graph = {
    'A': ['A', 'B', 'C'],  # self-loop + 2 ближайших
    'B': ['B', 'A', 'D'],
    'C': ['C', 'A', 'D'], 
    'D': ['D', 'B', 'C']
} 
 Расстояния между точками:
A→B: 1.0, A→C: 1.0, A→D: 1.41
 B→A: 1.0, B→D: 1.0, B→C: 1.41  
C→A: 1.0, C→D: 1.0, C→B: 1.41
 D→B: 1.0, D→C: 1.0, D→A: 1.41
 # Исходные координаты:
A: [0, 0] → 0.8
B: [1, 0] → 0.0  
C: [0, 1] → 0.8
D: [1, 1] → 0.3
Новые признаки:
new_features = [0.8, 0.0, 0.8, 0.3]
 Расстояния между новыми признаками:
 A(0.8) ↔ B(0.0): |0.8-0.0| = 0.8
 A(0.8) ↔ C(0.8): |0.8-0.8| = 0.0  
A(0.8) ↔ D(0.3): |0.8-0.3| = 0.5
B(0.0) ↔ C(0.8): |0.0-0.8| = 0.8
B(0.0) ↔ D(0.3): |0.0-0.3| = 0.3
 C(0.8) ↔ D(0.3): |0.8-0.3| = 0.5
new_graph = {
    'A': ['A', 'C', 'D'],  # Ближайшие по признакам: C(0.0), D(0.5)
    'B': ['B', 'D', 'A'],  # Ближайшие: D(0.3), A(0.8)  
    'C': ['C', 'A', 'D'],  # Ближайшие: A(0.0), D(0.5)
    'D': ['D', 'B', 'A']   # Ближайшие: B(0.3), A(0.5)
}
так как мы нашли новые связи и по другому строим 
TCL раздувает количесвто точек и пермножает на веса 
### Linked Dynamic Graph CNN
![[Pasted image 20251128102836.png]]
Основные отличия 
- Используется больше слоев 
- Не используется TLC, веса используется внутри Egne conv
- В сегментации используются все слои не только начальное
Модель становиться стабильнее четче и легче без TCL

